{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\anton\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loading the differents sets of data.\n",
    "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3, encoding=\"utf-8\" )\n",
    "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3, encoding=\"utf-8\" )\n",
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3, encoding=\"utf-8\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Getting the reviews for each set of data.\n",
    "x_train = train['review']\n",
    "x_test = test['review']\n",
    "x_unlabeled = unlabeled_train['review']\n",
    "\n",
    "#Getting the sentiment just for train. In unlabeled_train we don't\n",
    "#have sentiment column as same in test.\n",
    "y_train = train['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to clean the data. \n",
    "Removes html, special characters like punctuation and put all the words in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    clean_reviews = []\n",
    "    for i in range(len(review)):\n",
    "    # 1. Remove HTML\n",
    "        review_text = BeautifulSoup(review[i]).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "        words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            words = [w for w in words if not w in stops]\n",
    "    #\n",
    "        clean_reviews.append(words)\n",
    "    # 5. Return a list of reviews\n",
    "    return(clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file C:\\Users\\anton\\Anaconda2\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "#Cleaning the data with the reviews in each set of data\n",
    "x_train = review_to_wordlist(x_train)\n",
    "x_test = review_to_wordlist(x_test)\n",
    "x_unlabeled = review_to_wordlist(x_unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function that labelize each review into 'TRAIN_i' , 'TEST_i' and 'UNSUP_i' where i is an index for each observation. \n",
    "This procces is needed because in Doc2Vec it's necesary that each document, in this case\n",
    "review, needs to have a label associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def labelizeReviews(reviews, label_type):\n",
    "    labelized = []\n",
    "    for i,v in enumerate(reviews):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_concatenate = np.concatenate((x_train, x_unlabeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Doing labilize procces to each data set, specifying in the parameter \"label_type\"\n",
    "#what kind of data is the given data. \n",
    "x_train2 = labelizeReviews(x_concatenate, 'TRAIN')\n",
    "x_test2 = labelizeReviews(x_test, 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size = 50\n",
    "model= Doc2Vec(min_count=1, window=5, size=size, sample=1e-4, negative=5, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build vocab over all reviews\n",
    "model.build_vocab(x_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random as ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "ra.seed(12345)\n",
    "alpha = 0.025\n",
    "min_alpha = 0.001\n",
    "num_epochs = 5\n",
    "alpha_delta = (alpha - min_alpha) / num_epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    ra.shuffle(x_train2)\n",
    "    model.alpha = alpha\n",
    "    model.min_alpha = alpha\n",
    "    model.train(x_train2)\n",
    "    alpha -= alpha_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Infering over the test reviews in base the previous model built\n",
    "test_vectors = np.zeros((len(x_test),size))\n",
    "for i in range(len(x_test)):\n",
    "    test_vectors[i] = model.infer_vector(x_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Getting the train vectors from the model with the function docvecs. \n",
    "#Remember that when we concatenate train and unlabeled sets, the first \n",
    "#25,000 rows are from train set. The rest 50,000 rows are from the unlabeled set.\n",
    "train_vectors = np.zeros((len(x_train), size))\n",
    "for i in range(len(x_train)):\n",
    "    train_vectors[i] = model.docvecs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.save('my_model.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and prediciting sentimient with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest and extract predictions \n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "# Fitting the forest may take a few minutes\n",
    "print \"Fitting a random forest to labeled training data...\"\n",
    "forest = forest.fit(train_vectors,train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = forest.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":results})\n",
    "output.to_csv( \"Doc2Vec.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training and predicting sentiment with Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr = lgr.fit(train_vectors, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = lgr.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":results})\n",
    "output.to_csv( \"Doc2Vec_lgr.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is slightly better than Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
